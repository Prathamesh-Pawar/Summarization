{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Preprocessing:\n",
    "\n",
    "### Discription:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded data sets from Wikipedia using NLTK and used four pre-processing approaches to train our models for higher accuracy rates.<br>\n",
    "Pre-processing is crucial for cleaning and transforming data before analysis, and NLTK provides powerful tools for processing text data.<br> \n",
    "Combining these tools makes working with large text datasets easier and more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data collection we make a list of topics with complex scientific background and make use of the wikipedia package to download and store<br>\n",
    "the tiles, content and summary in a json file. Here is a code snipit showing how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import json\n",
    "from wikipedia.exceptions import WikipediaException\n",
    "# Set the language to English\n",
    "wikipedia.set_lang(\"en\")\n",
    "\n",
    "keywords = ['Hilbert\\'s fifth problem', 'P vs NP', 'Navier–Stokes existence and smoothness', 'Birch and Swinnerton-Dyer conjecture', 'Twin prime conjecture', ..]\n",
    "titles = []\n",
    "i = 0\n",
    "for keyword in keywords:\n",
    "    if i > 1000:\n",
    "        break\n",
    "    try:\n",
    "        pages = wikipedia.search(keyword, results=600)\n",
    "        for page in pages:\n",
    "            if page not in titles:\n",
    "                try:\n",
    "                    with open('10krun.json', 'r') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    summary = wikipedia.summary(page)\n",
    "                    content = wikipedia.page(page).content\n",
    "                    # Add the article data to the list\n",
    "                    data.append({\n",
    "                        \"topic\": page,\n",
    "                        \"summary\": summary,\n",
    "                        \"content\": content\n",
    "                    })\n",
    "                    with open('10krun.json', 'w') as f:\n",
    "                        json.dump(data, f)\n",
    "                        if len(data) > 1000:\n",
    "                            i = 1000\n",
    "                            break\n",
    "                except wikipedia.exceptions.DisambiguationError as e:\n",
    "                    # If the page is a disambiguation page, skip it\n",
    "                    continue \n",
    "    except wikipedia.exceptions.PageError as e:\n",
    "        # If no pages are found for the keyword, skip it\n",
    "        continue\n",
    "    except KeyError:\n",
    "\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize four distinct preprocessing approaches, including the Traditional, Custom, Raw, and Combined approaches. Each approach has its unique<br> strengths and benefits, allowing us to tailor our data processing for better evaluation and achieve higher accuracy rates in our models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize the content in its original form to establish a baseline that can aid in further ablation research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the article content actually looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "== Disciplines ==\n",
      "Physics and Astrophysics have played central roles in shaping our understanding of the universe through scientific observation and experiment. Physical cosmology was shaped through both mathematics and observation in an analysis of the whole universe. The universe is generally understood to have begun with the Big Bang, followed almost instantaneously by cosmic inflation, an expansion of space from which the universe is thought to have emerged 13.799 ± 0.021 billion years ago. Cosmogony studies the origin of the universe, and cosmography maps the features of the universe.\n",
      "In Diderot's Encyclopédie, cosmology is broken down into uranology (the science of the heavens), aerology (the science of the air), geology (the science of the continents), and hydrology (the science of waters).Metaphysical cosmology has also been described as the placing of humans in the universe in relationship to all other entities. This is exemplified by Marcus Aurelius's observation that a man's place in that relationship: \"He who does not know what the world is does not know where he is, and he who does not know for what purpose the world exists, does not know who he is, nor what the world is.\"\n",
      "\n",
      "\n",
      "== Discoveries ==\n",
      "\n",
      "\n",
      "=== Physical cosmology ===\n",
      "\n",
      "Physical cosmology is the branch of physics and astrophysics that deals with the study of the physical origins and evolution of the universe. It also includes the study of the nature of the universe on a large scale. In its earliest form, it was what is now known as \"celestial mechanics\", the study of the heavens. Greek philosophers Aristarchus of Samos, Aristotle, and Ptolemy proposed different cosmological theories. The geocentric Ptolemaic system was the prevailing theory until the 16th century when Nicolaus Copernicus, and subsequently Johannes Kepler and Galileo Galilei, proposed a heliocentric system. This is one of the most famous examples of epistemological rupture in physical cosmology.\n",
      "Isaac Newton's Principia Mathematica, published in 1687, was the first description of the law of universal gravitation. It provided a physical mechanism for Kepler's laws and also allowed the anomalies in previous systems, caused by gravitational interaction between the planets, to be resolved. A fundamental difference between Newton's cosmology and those preceding it was the Copernican principle—that the bodies on Earth obey the same physical laws as all celestial bodies. This was a crucial philosophical advance in physical cosmology.\n",
      "Modern scientific cosmology is usually considered to have begun in 1917 with Albert Einstein's publication of his final modification of general relativity in the paper \"Cosmological Considerations of the General Theory of Relativity\" (although this paper was not widely available outside of Germany until the end of World War I). General relativity prompted cosmogonists such as Willem de Sitter, Karl Schwarzschild, and Arthur Eddington to explore its astronomical ramifications, which enhanced the ability of astronomers to study very distant objects. Physicists began changing the assumption that the universe was static and unchanging. In 1922, Alexander Friedmann introduced the idea of an expanding universe that contained moving matter.\n",
      "\n",
      "In parallel to this dynamic approach to cosmology, one long-standing debate about the structure of the cosmos was coming to a climax - the Great Debate (1917 to 1922) - with early cosmologists such as Heber Curtis and Ernst Öpik determining that some nebulae seen in telescopes were separate galaxies far distant from our own. While Heber Curtis argued for the idea that spiral nebulae were star systems in their own right as island universes, Mount Wilson astronomer Harlow Shapley championed the model of a cosmos made up of the Milky Way star system only. This difference of ideas came to a climax with the organization of the Great Debate on 26 April 1920 at the meeting of the U.S. National Academy of Sciences in Washington, D.C. The debate was resolved when Edwin Hubble detected Cepheid Variables in the Andromeda Galaxy in 1923 and 1924. Their distance established spiral nebulae well beyond the edge of the Milky Way.\n",
      "Subsequent modelling of the universe explored the possibility that the cosmological constant, introduced by Einstein in his 1917 paper, may result in an expanding universe, depending on its value. Thus the Big Bang model was proposed by the Belgian priest Georges Lemaître in 1927 which was subsequently corroborated by Edwin Hubble's discovery of the redshift in 1929 and later by the discovery of the cosmic microwave background radiation by Arno Penzias and Robert Woodrow Wilson in 1964. These findings were a first step to rule out some of many alternative cosmologies.\n",
      "Since around 1990, several dramatic advances in observational cosmology have transformed cosmology from a largely speculative science into a predictive science with precise agreement between theory and observation. These advances include observations of the microwave background from the COBE, WMAP and Planck satellites, large new galaxy redshift surveys including 2dfGRS and SDSS, and observations of distant supernovae and gravitational lensing. These observations matched the predictions of the cosmic inflation theory, a modified Big Bang theory, and the specific version known as the Lambda-CDM model. This has led many to refer to modern times as the \"golden age of cosmology\".On 17 March 2014, astronomers at the Center for Astrophysics | Harvard & Smithsonian announced the detection of gravitational waves, providing strong evidence for inflation and the Big Bang. However, on 19 June 2014, lowered confidence in confirming the cosmic inflation findings was reported.On 1 December 2014, at the Planck 2014 meeting in Ferrara, Italy, astronomers reported that the universe is 13.8 billion years old and composed of 4.9% atomic matter, 26.6% dark matter and 68.5% dark energy.\n",
      "\n",
      "\n",
      "=== Religious or mythological cosmology ===\n",
      "\n",
      "Religious or mythological cosmology is a body of beliefs based on mythological, religious, and esoteric literature and traditions of creation and eschatology.\n",
      "\n",
      "\n",
      "=== Philosophical cosmology ===\n",
      "\n",
      "Cosmology deals with the world as the totality of space, time and all phenomena. Historically, it has had quite a broad scope, and in many cases was found in religion. In modern use metaphysical cosmology addresses questions about the Universe which are beyond the scope of science. It is distinguished from religious cosmology in that it approaches these questions using philosophical methods like dialectics. Modern metaphysical cosmology tries to address questions such as:\n",
      "What is the origin of the universe? What is its first cause? Is its existence necessary? (see monism, pantheism, emanationism and creationism)\n",
      "What are the ultimate material components of the universe? (see mechanism, dynamism, hylomorphism, atomism)\n",
      "What is the ultimate reason for the existence of the universe? Does the cosmos have a purpose? (see teleology)\n",
      "Does the existence of consciousness have a purpose? How do we know what we know about the totality of the cosmos? Does cosmological reasoning reveal metaphysical truths? (see epistemology)\n",
      "\n",
      "\n",
      "== Historical cosmologies ==\n",
      "\n",
      "Table notes: the term \"static\" simply means not expanding and not contracting. Symbol G represents Newton's gravitational constant; Λ (Lambda) is the cosmological constant.\n",
      "\n",
      "\n",
      "== See also ==\n",
      "\n",
      "\n",
      "== References ==\n",
      "\n",
      "\n",
      "== External links ==\n",
      "\n",
      "NASA/IPAC Extragalactic Database (NED) (NED-Distances)\n",
      "Cosmic Journey: A History of Scientific Cosmology Archived 21 October 2008 at the Wayback Machine from the American Institute of Physics\n",
      "Introduction to Cosmology David Lyth's lectures from the ICTP Summer School in High Energy Physics and Cosmology\n",
      "The Sophia Centre The Sophia Centre for the Study of Cosmology in Culture, University of Wales Trinity Saint David\n",
      "Genesis cosmic chemistry module\n",
      "\"The Universe's Shape\", BBC Radio 4 discussion with Sir Martin Rees, Julian Barbour and Janna Levin (In Our Time, 7 February 2002)\n"
     ]
    }
   ],
   "source": [
    "content = \"\\n\\n\\n== Disciplines ==\\nPhysics and Astrophysics have played central roles in shaping our understanding of the universe through scientific observation and experiment. Physical cosmology was shaped through both mathematics and observation in an analysis of the whole universe. The universe is generally understood to have begun with the Big Bang, followed almost instantaneously by cosmic inflation, an expansion of space from which the universe is thought to have emerged 13.799 \\u00b1 0.021 billion years ago. Cosmogony studies the origin of the universe, and cosmography maps the features of the universe.\\nIn Diderot's Encyclop\\u00e9die, cosmology is broken down into uranology (the science of the heavens), aerology (the science of the air), geology (the science of the continents), and hydrology (the science of waters).Metaphysical cosmology has also been described as the placing of humans in the universe in relationship to all other entities. This is exemplified by Marcus Aurelius's observation that a man's place in that relationship: \\\"He who does not know what the world is does not know where he is, and he who does not know for what purpose the world exists, does not know who he is, nor what the world is.\\\"\\n\\n\\n== Discoveries ==\\n\\n\\n=== Physical cosmology ===\\n\\nPhysical cosmology is the branch of physics and astrophysics that deals with the study of the physical origins and evolution of the universe. It also includes the study of the nature of the universe on a large scale. In its earliest form, it was what is now known as \\\"celestial mechanics\\\", the study of the heavens. Greek philosophers Aristarchus of Samos, Aristotle, and Ptolemy proposed different cosmological theories. The geocentric Ptolemaic system was the prevailing theory until the 16th century when Nicolaus Copernicus, and subsequently Johannes Kepler and Galileo Galilei, proposed a heliocentric system. This is one of the most famous examples of epistemological rupture in physical cosmology.\\nIsaac Newton's Principia Mathematica, published in 1687, was the first description of the law of universal gravitation. It provided a physical mechanism for Kepler's laws and also allowed the anomalies in previous systems, caused by gravitational interaction between the planets, to be resolved. A fundamental difference between Newton's cosmology and those preceding it was the Copernican principle\\u2014that the bodies on Earth obey the same physical laws as all celestial bodies. This was a crucial philosophical advance in physical cosmology.\\nModern scientific cosmology is usually considered to have begun in 1917 with Albert Einstein's publication of his final modification of general relativity in the paper \\\"Cosmological Considerations of the General Theory of Relativity\\\" (although this paper was not widely available outside of Germany until the end of World War I). General relativity prompted cosmogonists such as Willem de Sitter, Karl Schwarzschild, and Arthur Eddington to explore its astronomical ramifications, which enhanced the ability of astronomers to study very distant objects. Physicists began changing the assumption that the universe was static and unchanging. In 1922, Alexander Friedmann introduced the idea of an expanding universe that contained moving matter.\\n\\nIn parallel to this dynamic approach to cosmology, one long-standing debate about the structure of the cosmos was coming to a climax - the Great Debate (1917 to 1922) - with early cosmologists such as Heber Curtis and Ernst \\u00d6pik determining that some nebulae seen in telescopes were separate galaxies far distant from our own. While Heber Curtis argued for the idea that spiral nebulae were star systems in their own right as island universes, Mount Wilson astronomer Harlow Shapley championed the model of a cosmos made up of the Milky Way star system only. This difference of ideas came to a climax with the organization of the Great Debate on 26 April 1920 at the meeting of the U.S. National Academy of Sciences in Washington, D.C. The debate was resolved when Edwin Hubble detected Cepheid Variables in the Andromeda Galaxy in 1923 and 1924. Their distance established spiral nebulae well beyond the edge of the Milky Way.\\nSubsequent modelling of the universe explored the possibility that the cosmological constant, introduced by Einstein in his 1917 paper, may result in an expanding universe, depending on its value. Thus the Big Bang model was proposed by the Belgian priest Georges Lema\\u00eetre in 1927 which was subsequently corroborated by Edwin Hubble's discovery of the redshift in 1929 and later by the discovery of the cosmic microwave background radiation by Arno Penzias and Robert Woodrow Wilson in 1964. These findings were a first step to rule out some of many alternative cosmologies.\\nSince around 1990, several dramatic advances in observational cosmology have transformed cosmology from a largely speculative science into a predictive science with precise agreement between theory and observation. These advances include observations of the microwave background from the COBE, WMAP and Planck satellites, large new galaxy redshift surveys including 2dfGRS and SDSS, and observations of distant supernovae and gravitational lensing. These observations matched the predictions of the cosmic inflation theory, a modified Big Bang theory, and the specific version known as the Lambda-CDM model. This has led many to refer to modern times as the \\\"golden age of cosmology\\\".On 17 March 2014, astronomers at the Center for Astrophysics | Harvard & Smithsonian announced the detection of gravitational waves, providing strong evidence for inflation and the Big Bang. However, on 19 June 2014, lowered confidence in confirming the cosmic inflation findings was reported.On 1 December 2014, at the Planck 2014 meeting in Ferrara, Italy, astronomers reported that the universe is 13.8 billion years old and composed of 4.9% atomic matter, 26.6% dark matter and 68.5% dark energy.\\n\\n\\n=== Religious or mythological cosmology ===\\n\\nReligious or mythological cosmology is a body of beliefs based on mythological, religious, and esoteric literature and traditions of creation and eschatology.\\n\\n\\n=== Philosophical cosmology ===\\n\\nCosmology deals with the world as the totality of space, time and all phenomena. Historically, it has had quite a broad scope, and in many cases was found in religion. In modern use metaphysical cosmology addresses questions about the Universe which are beyond the scope of science. It is distinguished from religious cosmology in that it approaches these questions using philosophical methods like dialectics. Modern metaphysical cosmology tries to address questions such as:\\nWhat is the origin of the universe? What is its first cause? Is its existence necessary? (see monism, pantheism, emanationism and creationism)\\nWhat are the ultimate material components of the universe? (see mechanism, dynamism, hylomorphism, atomism)\\nWhat is the ultimate reason for the existence of the universe? Does the cosmos have a purpose? (see teleology)\\nDoes the existence of consciousness have a purpose? How do we know what we know about the totality of the cosmos? Does cosmological reasoning reveal metaphysical truths? (see epistemology)\\n\\n\\n== Historical cosmologies ==\\n\\nTable notes: the term \\\"static\\\" simply means not expanding and not contracting. Symbol G represents Newton's gravitational constant; \\u039b (Lambda) is the cosmological constant.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nNASA/IPAC Extragalactic Database (NED) (NED-Distances)\\nCosmic Journey: A History of Scientific Cosmology Archived 21 October 2008 at the Wayback Machine from the American Institute of Physics\\nIntroduction to Cosmology David Lyth's lectures from the ICTP Summer School in High Energy Physics and Cosmology\\nThe Sophia Centre The Sophia Centre for the Study of Cosmology in Culture, University of Wales Trinity Saint David\\nGenesis cosmic chemistry module\\n\\\"The Universe's Shape\\\", BBC Radio 4 discussion with Sir Martin Rees, Julian Barbour and Janna Levin (In Our Time, 7 February 2002)\"\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize a range of traditional preprocessing techniques, including stopword removal, punctuation filtering, and tokenization.<br>\n",
    " To perform these tasks, we rely on the Spacy library, which is renowned for its effectiveness in handling scientific text and <br> \n",
    " related terminology. \n",
    " \n",
    " Moreover, we employ the Term Frequency-Inverse Document Frequency (TFIDF) method to reduce the size of the document to 1500 tokens,<br>\n",
    "  which is essential for the model's optimal performance. Following this, we perform sentence segmentation and rank each sentence's <br> \n",
    "  importance using the TFIDF score. Finally, we only retain the highest ranking sentences that fit under the 1500 token limit, <br>\n",
    "  ensuring that only the most relevant and informative content is included in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def Traditional_approach(text):\n",
    "    '''\n",
    "    Input:\n",
    "        review: a string containing a review.\n",
    "    Output:\n",
    "        review_cleaned: a processed review. \n",
    "\n",
    "    '''\n",
    "    lst = re.findall('http://\\S+|https://\\S+', text)\n",
    "    for i in lst:\n",
    "        text = text.replace(i,'')\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = nlp(text)\n",
    "    tokens = [token.text for token in word_tokens ]\n",
    "    text_cleaned = []\n",
    "    for w in tokens:\n",
    "        if w not in stop_words:\n",
    "            text_cleaned.append(w)\n",
    "    filtered_text = [word for word in text_cleaned if '\\n' not in word]\n",
    "    filtered_text = [word for word in filtered_text if ' ' not in word]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "def tfidf_content(content):\n",
    "    # Load the input document and split it into sentences\n",
    "    if len(nltk.word_tokenize(content)) < 1500:\n",
    "        return content\n",
    "    else:\n",
    "        token = len(nltk.word_tokenize(content))\n",
    "        sentences = sentence_segmentation(content)\n",
    "        tfidf = TfidfVectorizer().fit_transform(sentences).toarray()\n",
    "        x = 1\n",
    "        para = \"\"\n",
    "        while token > 1500 :\n",
    "            N = len(sentences) - x \n",
    "            top_indices = np.argsort(tfidf.sum(axis=1))[::-1][:N]\n",
    "            # Concatenate the selected sentences into a single input sequence\n",
    "            para =  ' '.join([sentences[i] for i in top_indices])\n",
    "            token = len(nltk.word_tokenize(para))\n",
    "            x+=1\n",
    "        return para\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def sentence_segmentation(content):\n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    return sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is what Traditionaly processed data looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disciplines physics astrophysics played central roles shaping understanding universe scientific observation experiment physical <br>\n",
    " cosmology shaped mathematics observation analysis whole universe universe generally understood begun big bang followed almost <br>\n",
    " instantaneously cosmic inflation expansion space universe thought emerged 13799 ± 0021 billion years ago cosmogony studies <br>\n",
    " origin universe cosmography maps features universe diderots encyclopédie cosmology broken uranology science heavens aerology <br>\n",
    " science air geology science continents hydrology science watersmetaphysical cosmology also described placing humans universe <br>\n",
    " relationship entities exemplified marcus aureliuss observation mans place relationship know world know know purpose world exists <br>\n",
    " know world discoveries physical cosmology physical cosmology branch physics astrophysics deals study physical origins evolution <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custon Approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach we get rid of non-informational sections of the content like refernces, notes or symbols in some cases. before running TFIDF on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "def section_creator(article):\n",
    "# Download article from wikipedia\n",
    "    section_content = ''\n",
    "\n",
    "    # Split article into sections by headers\n",
    "    sections = {}\n",
    "    lines = article.split('\\n')\n",
    "    current_section = None\n",
    "    for line in lines:\n",
    "        if line.startswith('='):\n",
    "            if current_section is not None:\n",
    "                sections[current_section] = section_content.strip()\n",
    "            current_section = line.strip('= ')\n",
    "            section_content = ''\n",
    "        else:\n",
    "            section_content += line + '\\n'\n",
    "    if current_section is not None:\n",
    "        sections[current_section] = section_content.strip()\n",
    "        for head in sections.keys():\n",
    "            sections[head] = re.sub(r'\\s{1,}', ' ', sections[head]).replace('\\n', '')\n",
    "\n",
    "    return sections\n",
    "\n",
    "exclude = [\"See also\",\n",
    "\"References\",\n",
    "\"External links\",\n",
    "\"Notes\",\n",
    "\"Sources\",\n",
    "\"Further reading\",\n",
    "\"Bibliography\",\n",
    "\"Production\",\n",
    "\"Abstracting and indexing\",\n",
    "\"Examples\",\n",
    "\"Citations\",\n",
    "\"Nomenclature\",\n",
    "\"Evolution\",\n",
    "\"Uses\"]\n",
    "def exclusion(content):\n",
    "    new_content = {}\n",
    "    for title, value in content.items():\n",
    "        if title not in exclude:\n",
    "            new_content[title] = value\n",
    "    return new_content\n",
    "\n",
    "\n",
    "def custom_approach(content):\n",
    "    con_dict = section_creator(content)\n",
    "    con_dict = exclusion(con_dict)\n",
    "    total = \"\"\n",
    "    for key in con_dict.keys():\n",
    "        total += con_dict[key]\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here What custon approach looks like "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Physics and Astrophysics have played central roles in shaping our understanding of the universe through scientific observation <br> \n",
    "and experiment. Physical cosmology was shaped through both mathematics and observation in an analysis of the whole universe. The <br>\n",
    "universe is generally understood to have begun with the Big Bang, followed almost instantaneously by cosmic inflation, an expansion <br>\n",
    "of space from which the universe is thought to have emerged 13.799 ± 0.021 billion years ago. Cosmogony studies the origin of the <br>\n",
    "universe, and cosmography maps the features of the universe. In Diderot\\'s Encyclopédie, cosmology is broken down into uranology <br>\n",
    "(the science of the heavens), aerology (the science of the air), geology (the science of the continents), and hydrology (the science<br>\n",
    " of waters).Metaphysical cosmology has also been described as the placing of humans in the universe in relationship to all other entities.<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this is apart of the abliation stuy where we use both traditional and custum approach of pre-processing on this content<br> for a better comparision and to see if it can give a higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_approach(content):\n",
    "    return Traditional_approach(custom_approach(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is what the combined approach looks like "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'physics astrophysics played central roles shaping understanding universe scientific observation experiment physical cosmology<br> \n",
    "shaped mathematics observation analysis whole universe universe generally understood begun big bang followed almost instantaneously<br>\n",
    " cosmic inflation expansion space universe thought emerged 13799 ± 0021 billion years ago cosmogony studies origin universe cosmography<br>\n",
    "  maps features universe diderots encyclopédie cosmology broken uranology science heavens aerology science air geology science <br>\n",
    "  continents hydrology science watersmetaphysical cosmology also described placing humans universe relationship entities exemplified<br>\n",
    "   marcus aureliuss observation mans place relationship know world know know purpose world exists know world isphysical cosmology branch<br>\n",
    "    physics astrophysics deals study physical origins evolution universe also includes study nature universe large scale earliest form<br>\n",
    "     known celestial mechanics study heavens greek philosophers aristarchus samos aristotle ptolemy proposed different cosmological <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discription:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize our text summarization results, we leveraged the power of two rival companies - Google and Facebook - and <br>\n",
    "fine-tuned their respective models using transfer learning on our preprocessed data. This allowed us to achieve superior<br>\n",
    " performance and accuracy in our summarization tasks, enabling us to extract the most important insights and information <br>from our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"3500_train_trad.json\"}\n",
    "dataset = load_dataset(\"PrathameshPawar/10ktesttrain\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"content\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=8192, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset['train'].train_test_split(test_size=0.1)\n",
    "dataset_train = dataset_train.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')  \n",
    "    torch.cuda.set_device(device)  \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n",
    "\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name,)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name,max_position_embeddings=8192).to(device)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"pegasus_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train[\"train\"],\n",
    "    eval_dataset=dataset_train[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](bart_custom.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We shall use ROUGE score as an evaluation metric for the summarization task.\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation): ROUGE is a widely used evaluation metric for text summarization. It measures the overlap between the generated summary and the reference summaries in terms of n-gram (unigram, bigram, etc.) matches, as well as the length of the summary. Higher ROUGE scores indicate better summary quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def split_into_chunks(text, max_length):\n",
    "    \"\"\"\n",
    "    Splits a string into chunks of text with complete sentences, where each chunk\n",
    "    has a maximum length of `max_length` characters.\n",
    "    \"\"\"\n",
    "    sentences = re.findall(r'[^\\n.!?]+[.!?]', text)  # Split into sentences\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) <= max_length:\n",
    "            # If adding the sentence doesn't exceed max_length, add to current chunk\n",
    "            current_chunk += sentence\n",
    "        else:\n",
    "            # If adding the sentence exceeds max_length, start a new chunk\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    \n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "def get_chunks(input_text):\n",
    "    max_length = 1025\n",
    "    chunks = split_into_chunks(input_text, max_length)\n",
    "    \n",
    "    summary_temps=[]\n",
    "    \n",
    "    for i in chunks:\n",
    "        summary_temps.append(summarizer(i,max_length=32))\n",
    "        \n",
    "    summary_temps_ = [i[0]['summary_text'] for i in summary_temps]\n",
    "        \n",
    "    return '. '.join(summary_temps_)\n",
    "\n",
    "from rouge import Rouge\n",
    "# Initialize ROUGE\n",
    "rouge = Rouge()\n",
    "\n",
    "def rouge_score_generation(generated_summary,reference_summary):\n",
    "\n",
    "#     # Example generated and reference summaries\n",
    "#     generated_summary = x#content_sustom_summary\n",
    "#     reference_summary = test_dataset['summary'][0]\n",
    "#     # Compute ROUGE scores\n",
    "    scores = rouge.get_scores(generated_summary, reference_summary)\n",
    "\n",
    "    # Extract relevant ROUGE scores\n",
    "    rouge_1 = scores[0]['rouge-1']['f']\n",
    "    rouge_2 = scores[0]['rouge-2']['f']\n",
    "    rouge_l = scores[0]['rouge-l']['f']\n",
    "\n",
    "    # Print ROUGE scores\n",
    "    print(\"ROUGE-1: {:.2f}\".format(rouge_1 * 100))\n",
    "    print(\"ROUGE-2: {:.2f}\".format(rouge_2 * 100))\n",
    "    print(\"ROUGE-L: {:.2f}\".format(rouge_l * 100))\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/prathameshpawar/.cache/huggingface/datasets/PrathameshPawar___json/PrathameshPawar--summary_2k-c9ec564ecb7c9e74/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f058e75286c4111bc100b098528d6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "data_files = {\"test\": \"1000_test.json\"}\n",
    "\n",
    "dataset = load_dataset(\"PrathameshPawar/summary_2k\", data_files=data_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['topic', 'summary', 'content', 'content_traditional', 'custom_approach', 'combined_approach'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bart-Base model shall be used as a control summary to evaluate the score against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prathameshpawar/miniforge3/envs/mlp/lib/python3.8/site-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 16.04\n",
      "ROUGE-2: 1.97\n",
      "ROUGE-L: 14.33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\",)\n",
    "\n",
    "bart_base_summary = get_chunks(dataset['test']['content'][0])\n",
    "\n",
    "rouge_score_generation(dataset['test']['summary'][0],bart_base_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We concluded that Bart-base model finetuned on custom preprocessing data approach as the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"PrathameshPawar/bart_custom\",)\n",
    "\n",
    "bart_custom_summary = get_chunks(dataset['test']['custom_approach'][0])\n",
    "\n",
    "bart_custom_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 21.21\n",
      "ROUGE-2: 5.26\n",
      "ROUGE-L: 21.21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_score_generation(dataset['test']['summary'][0],bart_custom_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegasus model shall be used as a control summary to evaluate the score against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 19.91\n",
      "ROUGE-2: 3.47\n",
      "ROUGE-L: 17.59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"google/pegasus-arxiv\",)\n",
    "\n",
    "bart_base_summary = get_chunks(dataset['test']['content'][0])\n",
    "\n",
    "rouge_score_generation(dataset['test']['summary'][0],bart_base_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We concluded that Pegasus model finetuned on custom preprocessing data approach as the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"PrathameshPawar/pegasus_custom\",)\n",
    "\n",
    "pegasus_custom_summary = get_chunks(dataset['test']['custom_approach'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 21.46\n",
      "ROUGE-2: 3.87\n",
      "ROUGE-L: 16.31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_score_generation(dataset['test']['summary'][0],pegasus_custom_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplification ###\n",
    "Using the Hugging Face's Transformer, we use Google's pretrained model \"T5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prathameshpawar/miniforge3/envs/mlp/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  physics astrophysics played central roles shaping understanding universe scientific observation experiment physical cosmology \n",
      "shaped mathematics observation analysis whole universe universe generally understood begun big bang followed almost instantaneously\n",
      " cosmic inflation expansion space universe thought emerged 13799 ± 0021 billion years ago cosmogony studies origin universe cosmography<br>\n",
      "  maps features universe diderots encyclopédie cosmology broken uranology science heavens aerology science air geology science \n",
      "  continents hydrology science watersmetaphysical cosmology also described placing humans universe relationship entities exemplified\n",
      "   marcus aureliuss observation mans place relationship know world know know purpose world exists know world isphysical cosmology branch\n",
      "    physics astrophysics deals study physical origins evolution universe also includes study nature universe large scale earliest form\n",
      "     known celestial mechanics study heavens greek philosophers aristarchus samos aristotle ptolemy proposed different cosmological\n",
      "Sample output:  cosmology branch physics astrophysics played central roles shaping understanding universe universe observation experiment physical cosmology shaped mathematics observation analysis whole universe universe generally understood begun big bang followed almost instantaneously cosmic inflation expansion universe thought emerged 13799  0021 billion years ago cosmogony studies origin universe cosmology branch physics astrophysics played central roles shaping understanding universe universe observation experiment physical cosmology shaped mathematics observation analysis whole universe universe generally understood begun big bang followed almost instantaneously cosmic\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def simplify_text(text):\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_length=128, num_beams=4, early_stopping=True)\n",
    "    simplified_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return simplified_text\n",
    "\n",
    "text = \"\"\"physics astrophysics played central roles shaping understanding universe scientific observation experiment physical cosmology \n",
    "shaped mathematics observation analysis whole universe universe generally understood begun big bang followed almost instantaneously\n",
    " cosmic inflation expansion space universe thought emerged 13799 ± 0021 billion years ago cosmogony studies origin universe cosmography<br>\n",
    "  maps features universe diderots encyclopédie cosmology broken uranology science heavens aerology science air geology science \n",
    "  continents hydrology science watersmetaphysical cosmology also described placing humans universe relationship entities exemplified\n",
    "   marcus aureliuss observation mans place relationship know world know know purpose world exists know world isphysical cosmology branch\n",
    "    physics astrophysics deals study physical origins evolution universe also includes study nature universe large scale earliest form\n",
    "     known celestial mechanics study heavens greek philosophers aristarchus samos aristotle ptolemy proposed different cosmological\"\"\"\n",
    "simplified_text = simplify_text(text)\n",
    "print(\"Original text: \", text)\n",
    "print(\"Sample output: \", simplified_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the ROUGE-2 score for the overlap of bigrams between system and reference summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 score: 0.575757571657484\n",
      "ROUGE-2 score: 0.4878048741225461\n",
      "ROUGE-l score: 0.575757571657484\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "reference_text = text\n",
    "predicted_text = simplified_text\n",
    "\n",
    "scores = rouge.get_scores(reference_text, predicted_text, avg=True)\n",
    "rouge1_score = scores[\"rouge-1\"][\"f\"]\n",
    "rouge2_score = scores[\"rouge-2\"][\"f\"]\n",
    "rougel_score = scores[\"rouge-l\"][\"f\"]\n",
    "\n",
    "print(\"ROUGE-1 score:\", rouge1_score)\n",
    "print(\"ROUGE-2 score:\", rouge2_score)\n",
    "print(\"ROUGE-l score:\", rougel_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original summary:  Metagenomics is the study of genetic material recovered directly from environmental or clinical samples by a method called sequencing. The broad field may also be referred to as environmental genomics, ecogenomics, community genomics or microbiomics.\n",
      "While traditional microbiology and microbial genome sequencing and genomics rely upon cultivated clonal cultures, early environmental gene sequencing cloned specific genes (often the 16S rRNA gene) to produce a profile of diversity in a natural sample. Such work revealed that the vast majority of microbial biodiversity had been missed by cultivation-based methods.Because of its ability to reveal the previously hidden diversity of microscopic life, metagenomics offers a powerful lens for viewing the microbial world that has the potential to revolutionize understanding of the entire living world. As the price of DNA sequencing continues to fall, metagenomics now allows microbial ecology to be investigated at a much greater scale and detail than before. Recent studies use either \"shotgun\" or PCR directed sequencing to get largely unbiased samples of all genes from all the members of the sampled communities.\n",
      "Simplified summary:  early environmental gene sequencing cloned specific genes (often the 16S rRNA gene) to produce a profile of biodiversity in a natural sample. Such work revealed that the vast majority of microbial biodiversity had been missed by cultivation-based methods. Metagenomics offers a powerful lens for viewing the microbial world that has potential to revolutionize understanding of the entire living world.\n",
      "ROUGE-2 score: 0.4628820922873324\n",
      "Original summary:  Elena Litchman is a professor of aquatic ecology at Michigan State University known for her research on the consequences of global environmental change on phytoplankton.\n",
      "Simplified summary:  best known for her research on the consequences of global environmental change on phytoplankton. Elena Litchman is a professor of aquatic ecology at Michigan State University known for her research on the consequences of global environmental change on phytoplankton.\n",
      "ROUGE-2 score: 0.959999995008\n",
      "Original summary:  Heredity, also called inheritance or biological inheritance, is the passing on of traits from parents to their offspring; either through asexual reproduction or sexual reproduction, the offspring cells or organisms acquire the genetic information of their parents. Through heredity, variations between individuals can accumulate and cause species to evolve by natural selection. The study of heredity in biology is genetics.\n",
      "Simplified summary:  the offspring acquire the traits of their parents. Through heredity, variations between individuals can accumulate and cause species to evolve by natural selection.\n",
      "ROUGE-2 score: 0.469135798512422\n",
      "Original summary:  In ecology, r/K selection theory relates to the selection of combinations of traits in an organism that trade off between quantity and quality of offspring. The focus on either an increased quantity of offspring at the expense of individual parental investment of r-strategists, or on a reduced quantity of offspring with a corresponding increased parental investment of K-strategists, varies widely, seemingly to promote success in particular environments. The concepts of quantity or quality offspring are sometimes referred to as \"cheap\" or \"expensive\", a comment on the expendable nature of the offspring and parental commitment made. The stability of the environment can predict if many expendable offspring are made or if fewer offspring of higher quality would lead to higher reproductive success. An unstable environment would encourage the parent to make many offspring, because the likelihood of all (or the majority) of them surviving to adulthood is slim. In contrast, more stable environments allow parents to confidently invest in one offspring because they are more likely to survive to adulthood.\n",
      "The terminology of r/K-selection was coined by the ecologists Robert MacArthur and E. O. Wilson in 1967 based on their work on island biogeography; although the concept of the evolution of life history strategies has a longer history (see e.g. plant strategies).\n",
      "The theory was popular in the 1970s and 1980s, when it was used as a heuristic device, but lost importance in the early 1990s, when it was criticized by several empirical studies. A life-history paradigm has replaced the r/K selection paradigm, but continues to incorporate its important themes as a subset of life history theory. Some scientists now prefer to use the terms fast versus slow life history as a replacement for, respectively, r versus K reproductive strategy.\n",
      "\n",
      "\n",
      "Simplified summary:  an unstable environment would encourage the parent to make many expendable offspring, because the likelihood of all (or most) of them surviving to adulthood is slim. An unstable environment would encourage the parent to make many expendable offspring, because the likelihood of all (or most) of them surviving to adulthood is slim. An unstable environment would encourage the parent to make many expendable offspring, because the likelihood of all (or most) of them surviving to adulthood is slim.\n",
      "ROUGE-2 score: 0.14715718899251687\n",
      "Original summary:  Fire adaptations are life history traits of plants and animals that help them survive wildfire or to use resources created by wildfire. These traits can help plants and animals increase their survival rates during a fire and/or reproduce offspring after a fire. Both plants and animals have multiple strategies for surviving and reproducing after fire.\n",
      "Simplified summary:  Fire adaptations are life history traits of plants and animals that help them survive wildfire or to use resources created by wildfire. Fire adaptations are life history traits of plants and animals that help them survive wildfire or to use resources created by wildfire.\n",
      "ROUGE-2 score: 0.5915492914977187\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('1000_test.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "for article in data[:5]:\n",
    "    summary = article['summary']\n",
    "    simplified_summary = simplify_text(summary)\n",
    "    print(\"Original summary: \", summary)\n",
    "    print(\"Simplified summary: \", simplified_summary)\n",
    "    scores = rouge.get_scores(summary, simplified_summary, avg=True)\n",
    "    rouge2_score = scores[\"rouge-2\"][\"f\"]\n",
    "\n",
    "    # print ROUGE-2 score\n",
    "    print(\"ROUGE-2 score:\", rouge2_score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
